{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Meme Images into Hateful or Not-Hateful \n",
    "### Model 2: Training MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = \"img\"\n",
    "train_folder = \"train_data\"\n",
    "validation_folder = \"validation_data\"\n",
    "\n",
    "train_dir = os.path.join(source_folder, train_folder)\n",
    "validation_dir = os.path.join(source_folder, validation_folder)\n",
    "\n",
    "# Various Directory Paths\n",
    "train_hateful_memes_dir = os.path.join(train_dir, 'hateful_memes')\n",
    "train_not_hateful_memes_dir = os.path.join(train_dir, 'not_hateful_memes')\n",
    "validation_hateful_memes_dir = os.path.join(validation_dir, 'hateful_memes')\n",
    "validation_not_hateful_memes_dir = os.path.join(validation_dir, 'not_hateful_memes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Checking Count of Images in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training hateful images: 2400\n",
      "total training not hateful images: 2400\n",
      "total validation hateful images: 600\n",
      "total validation not hateful images: 600\n"
     ]
    }
   ],
   "source": [
    "print('total training hateful images:', len(os.listdir(train_hateful_memes_dir)))\n",
    "print('total training not hateful images:', len(os.listdir(train_not_hateful_memes_dir)))\n",
    "print('total validation hateful images:', len(os.listdir(validation_hateful_memes_dir)))\n",
    "print('total validation not hateful images:', len(os.listdir(validation_not_hateful_memes_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Performing Data-Augmentation using ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data-augmentation parameters to ImageDataGenerator for Training set\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Setting up Training and Validation Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches of 1 using train_datagen generator\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        classes = ['hateful_memes', 'not_hateful_memes'], #The Classification\n",
    "        target_size=(224, 224),  # All images will be resized to 224x224\n",
    "        batch_size=1,\n",
    "        class_mode='binary') # Use binary labels\n",
    "\n",
    "\n",
    "# Flow validation images in batches of 1 using valid_datagen generator\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,  # This is the source directory for validating images\n",
    "        classes = ['hateful_memes', 'not_hateful_memes'], #The Classification\n",
    "        target_size=(224, 224),  # All images will be resized to 224x224\n",
    "        batch_size=1,\n",
    "        class_mode='binary', # Use binary labels\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Setting up the MobileNetV2 Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2259265 (8.62 MB)\n",
      "Trainable params: 1281 (5.00 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(MobileNetV2(include_top = False, weights=\"imagenet\", input_shape=(224, 224, 3)))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Choosing the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss = 'binary_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Start the Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 107s 22ms/step - loss: 0.8196 - accuracy: 0.5337 - val_loss: 0.8297 - val_accuracy: 0.5400\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 104s 22ms/step - loss: 0.7994 - accuracy: 0.5506 - val_loss: 0.7519 - val_accuracy: 0.5733\n",
      "Epoch 3/10\n",
      "4800/4800 [==============================] - 105s 22ms/step - loss: 0.7837 - accuracy: 0.5723 - val_loss: 0.7601 - val_accuracy: 0.5575\n",
      "Epoch 4/10\n",
      "4800/4800 [==============================] - 110s 23ms/step - loss: 0.7828 - accuracy: 0.5758 - val_loss: 0.8938 - val_accuracy: 0.5317\n",
      "Epoch 5/10\n",
      "4800/4800 [==============================] - 104s 22ms/step - loss: 0.7778 - accuracy: 0.5792 - val_loss: 0.7913 - val_accuracy: 0.5650\n",
      "Epoch 6/10\n",
      "4800/4800 [==============================] - 105s 22ms/step - loss: 0.7843 - accuracy: 0.5815 - val_loss: 0.8032 - val_accuracy: 0.5717\n",
      "Epoch 7/10\n",
      "4800/4800 [==============================] - 108s 22ms/step - loss: 0.7823 - accuracy: 0.5919 - val_loss: 0.8045 - val_accuracy: 0.5625\n",
      "Epoch 8/10\n",
      "4800/4800 [==============================] - 109s 23ms/step - loss: 0.7705 - accuracy: 0.5894 - val_loss: 0.8396 - val_accuracy: 0.5708\n",
      "Epoch 9/10\n",
      "4800/4800 [==============================] - 108s 22ms/step - loss: 0.7771 - accuracy: 0.5846 - val_loss: 1.0548 - val_accuracy: 0.5467\n",
      "Epoch 10/10\n",
      "4800/4800 [==============================] - 107s 22ms/step - loss: 0.7722 - accuracy: 0.5992 - val_loss: 0.8392 - val_accuracy: 0.5642\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=4800,  \n",
    "      epochs=10,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Start Validation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 21s 18ms/step - loss: 0.8392 - accuracy: 0.5642\n",
      "Validation Loss: 0.8392002582550049\n",
      "Validation Accuracy: 0.5641666650772095\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_generator)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Classification_MobileNet.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Testing the Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_image_directory = \"\" #Insert the Folder Path containing the images\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List all files in the directory\n",
    "image_files = [f for f in os.listdir(local_image_directory) if os.path.isfile(os.path.join(local_image_directory, f))]\n",
    "\n",
    "for fn in image_files:\n",
    "    # Predicting images\n",
    "    path = os.path.join(local_image_directory, fn)  # Updated to use os.path.join for the file path\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=1)\n",
    "    if classes[0] < 0.5:\n",
    "        print(fn + \" is a Hateful Meme\")\n",
    "    else:\n",
    "        print(fn + \" is a Not Hateful Meme\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
